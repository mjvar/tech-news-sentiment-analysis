{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bibliographic-jefferson",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Tech News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tender-spring",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to /Users/joey/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/joey/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/joey/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/joey/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /Users/joey/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.download([\"names\",\n",
    "     \"stopwords\",\n",
    "     \"averaged_perceptron_tagger\",\n",
    "     \"vader_lexicon\",\n",
    "     \"punkt\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "promotional-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(\"training.csv\", header=None)\n",
    "devt_set = pd.read_csv(\"development.csv\", header=None)\n",
    "test_set = pd.read_csv(\"test.csv\", header=None, encoding= 'unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "affecting-mistress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2346\n",
      "1500\n",
      "774\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))\n",
    "print(len(devt_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "absolute-nevada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "stock-crossing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.493, 'pos': 0.507, 'compound': 0.7351}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(train_set[1][164])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "homeless-rally",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks the sentiment of a single string of text, ranking it positive, negative, or neutral.\n",
    "def check_sentiment(string_to_analyze):\n",
    "    scores = sia.polarity_scores(string_to_analyze)\n",
    "    print(scores)\n",
    "    neg_neu_pos = [scores['neg'],scores['neu'],scores['pos']]\n",
    "    highest_score = neg_neu_pos.index(max(neg_neu_pos))\n",
    "    return_values = [\"negative\", \"neutral\", \"positive\"]\n",
    "    return return_values[highest_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "tropical-calculation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_sentiment(train_set[1][164])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "played-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring each individual string from the corpus\n",
    "train_list = []\n",
    "for x in train_set[1]:\n",
    "    train_list.append(check_sentiment(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "proprietary-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we score the guesses of our sentiment analyzer.\n",
    "# The score is based on the distance of the guesses, e.g.\n",
    "# if the answer is \"negative\" but our system guesses \"positive\",\n",
    "# the penalty is larger than, say, if the answer was \"neutral\"\n",
    "# but our system guessed \"positive\".\n",
    "total_score = len(train_list)\n",
    "score_dict = {\"negative\":-0.5, \"neutral\":0, \"positive\":0.5}\n",
    "for index, value in enumerate(train_list):\n",
    "    total_score -= abs(score_dict[value] - score_dict[train_set[0][index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "communist-radiation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8589087809036658\n"
     ]
    }
   ],
   "source": [
    "print(total_score/len(train_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "alert-watch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8388746803069054\n"
     ]
    }
   ],
   "source": [
    "# Filters out the POS to only have nouns\n",
    "pos_tags_to_keep = [\"NN\",\"NNP\",\"NNS\",\"NNPS\"]\n",
    "noun_list = []\n",
    "for entry in train_set[1]:\n",
    "    tokenized = nltk.word_tokenize(entry)\n",
    "    tagged = nltk.pos_tag(tokenized)\n",
    "    new_phrase = \"\"\n",
    "    for word in tagged:\n",
    "        if word[1] in pos_tags_to_keep:\n",
    "            new_phrase += word[0] + \" \"\n",
    "    noun_list.append(new_phrase)\n",
    "\n",
    "#Get sentiment for train set with only nouns:\n",
    "train_list_noun = []\n",
    "for x in noun_list:\n",
    "    train_list_noun.append(check_sentiment(x))\n",
    "total_score = len(train_list_noun)\n",
    "score_dict = {\"negative\":-0.5, \"neutral\":0, \"positive\":0.5}\n",
    "for index, value in enumerate(train_list_noun):\n",
    "    total_score -= abs(score_dict[value] - score_dict[train_set[0][index]])\n",
    "\n",
    "#Accuracy score with only nouns\n",
    "print(total_score/len(train_list_noun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "historic-burlington",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7664109121909634\n"
     ]
    }
   ],
   "source": [
    "# Filters out the POS to only have adjectives\n",
    "pos_tags_to_keep = [\"JJS\",\"JJR\",\"JJ\"]\n",
    "adj_list = []\n",
    "for entry in train_set[1]:\n",
    "    tokenized = nltk.word_tokenize(entry)\n",
    "    tagged = nltk.pos_tag(tokenized)\n",
    "    new_phrase = \"\"\n",
    "    for word in tagged:\n",
    "        if word[1] in pos_tags_to_keep:\n",
    "            new_phrase += word[0] + \" \"\n",
    "    adj_list.append(new_phrase)\n",
    "\n",
    "#Get sentiment for train set with only adjectives:\n",
    "train_list_adj = []\n",
    "for x in adj_list:\n",
    "    train_list_adj.append(check_sentiment(x))\n",
    "total_score = len(train_list_adj)\n",
    "score_dict = {\"negative\":-0.5, \"neutral\":0, \"positive\":0.5}\n",
    "for index, value in enumerate(train_list_adj):\n",
    "    total_score -= abs(score_dict[value] - score_dict[train_set[0][index]])\n",
    "\n",
    "#Accuracy score with only adjectives\n",
    "print(total_score/len(train_list_adj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aging-northern",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8478260869565217\n"
     ]
    }
   ],
   "source": [
    "# Filters out the POS to only have adjectives AND nouns\n",
    "pos_tags_to_keep = [\"JJS\",\"JJR\",\"JJ\",\"NN\",\"NNP\",\"NNS\",\"NNPS\"]\n",
    "adj_noun_list = []\n",
    "for entry in train_set[1]:\n",
    "    tokenized = nltk.word_tokenize(entry)\n",
    "    tagged = nltk.pos_tag(tokenized)\n",
    "    new_phrase = \"\"\n",
    "    for word in tagged:\n",
    "        if word[1] in pos_tags_to_keep:\n",
    "            new_phrase += word[0] + \" \"\n",
    "    adj_noun_list.append(new_phrase)\n",
    "\n",
    "#Get sentiment for train set with only adjectives:\n",
    "train_list_adj_noun = []\n",
    "for x in adj_noun_list:\n",
    "    train_list_adj_noun.append(check_sentiment(x))\n",
    "total_score = len(train_list_adj_noun)\n",
    "score_dict = {\"negative\":-0.5, \"neutral\":0, \"positive\":0.5}\n",
    "for index, value in enumerate(train_list_adj_noun):\n",
    "    total_score -= abs(score_dict[value] - score_dict[train_set[0][index]])\n",
    "\n",
    "#Accuracy score with only adjectives\n",
    "print(total_score/len(train_list_adj_noun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "spread-scott",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.025, 'neu': 0.924, 'pos': 0.051, 'compound': 0.4019}\n",
      "{'neg': 0.148, 'neu': 0.742, 'pos': 0.11, 'compound': -0.802}\n",
      "{'neg': 0.047, 'neu': 0.888, 'pos': 0.065, 'compound': 0.3182}\n",
      "{'neg': 0.217, 'neu': 0.749, 'pos': 0.034, 'compound': -0.7964}\n",
      "{'neg': 0.048, 'neu': 0.778, 'pos': 0.174, 'compound': 0.875}\n",
      "{'neg': 0.0, 'neu': 0.969, 'pos': 0.031, 'compound': 0.0772}\n",
      "{'neg': 0.08, 'neu': 0.844, 'pos': 0.075, 'compound': 0.3612}\n",
      "{'neg': 0.019, 'neu': 0.889, 'pos': 0.092, 'compound': 0.7579}\n",
      "{'neg': 0.169, 'neu': 0.653, 'pos': 0.178, 'compound': -0.2732}\n",
      "{'neg': 0.031, 'neu': 0.827, 'pos': 0.142, 'compound': 0.9657}\n",
      "{'neg': 0.111, 'neu': 0.795, 'pos': 0.094, 'compound': -0.1779}\n",
      "{'neg': 0.0, 'neu': 0.948, 'pos': 0.052, 'compound': 0.5574}\n",
      "{'neg': 0.054, 'neu': 0.821, 'pos': 0.126, 'compound': 0.9201}\n",
      "{'neg': 0.053, 'neu': 0.781, 'pos': 0.166, 'compound': 0.8779}\n",
      "{'neg': 0.099, 'neu': 0.749, 'pos': 0.152, 'compound': 0.8519}\n",
      "{'neg': 0.147, 'neu': 0.701, 'pos': 0.153, 'compound': 0.6369}\n",
      "{'neg': 0.077, 'neu': 0.71, 'pos': 0.214, 'compound': 0.9136}\n",
      "{'neg': 0.033, 'neu': 0.691, 'pos': 0.276, 'compound': 0.9937}\n",
      "{'neg': 0.038, 'neu': 0.962, 'pos': 0.0, 'compound': -0.128}\n",
      "{'neg': 0.112, 'neu': 0.761, 'pos': 0.127, 'compound': 0.4215}\n",
      "{'neg': 0.04, 'neu': 0.632, 'pos': 0.328, 'compound': 0.9887}\n",
      "{'neg': 0.091, 'neu': 0.863, 'pos': 0.047, 'compound': -0.4404}\n",
      "{'neg': 0.055, 'neu': 0.861, 'pos': 0.084, 'compound': 0.6908}\n",
      "{'neg': 0.132, 'neu': 0.691, 'pos': 0.178, 'compound': 0.7184}\n",
      "{'neg': 0.0, 'neu': 0.896, 'pos': 0.104, 'compound': 0.8316}\n",
      "{'neg': 0.026, 'neu': 0.951, 'pos': 0.023, 'compound': 0.1531}\n",
      "{'neg': 0.079, 'neu': 0.8, 'pos': 0.121, 'compound': 0.7783}\n",
      "{'neg': 0.101, 'neu': 0.754, 'pos': 0.145, 'compound': 0.8957}\n",
      "{'neg': 0.081, 'neu': 0.7, 'pos': 0.219, 'compound': 0.9794}\n",
      "{'neg': 0.034, 'neu': 0.873, 'pos': 0.093, 'compound': 0.8271}\n",
      "{'neg': 0.036, 'neu': 0.779, 'pos': 0.185, 'compound': 0.9753}\n",
      "{'neg': 0.069, 'neu': 0.771, 'pos': 0.161, 'compound': 0.5423}\n",
      "{'neg': 0.109, 'neu': 0.696, 'pos': 0.195, 'compound': 0.8658}\n",
      "{'neg': 0.052, 'neu': 0.82, 'pos': 0.128, 'compound': 0.8591}\n",
      "{'neg': 0.0, 'neu': 0.916, 'pos': 0.084, 'compound': 0.5267}\n",
      "{'neg': 0.155, 'neu': 0.795, 'pos': 0.05, 'compound': -0.6597}\n",
      "{'neg': 0.037, 'neu': 0.889, 'pos': 0.074, 'compound': 0.5574}\n",
      "{'neg': 0.081, 'neu': 0.662, 'pos': 0.257, 'compound': 0.9382}\n",
      "{'neg': 0.153, 'neu': 0.74, 'pos': 0.107, 'compound': -0.7003}\n",
      "{'neg': 0.103, 'neu': 0.838, 'pos': 0.059, 'compound': -0.25}\n",
      "{'neg': 0.065, 'neu': 0.675, 'pos': 0.26, 'compound': 0.7003}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.109, 'neu': 0.72, 'pos': 0.17, 'compound': 0.4767}\n",
      "{'neg': 0.065, 'neu': 0.899, 'pos': 0.036, 'compound': -0.4939}\n",
      "{'neg': 0.104, 'neu': 0.747, 'pos': 0.149, 'compound': 0.4939}\n",
      "{'neg': 0.23, 'neu': 0.757, 'pos': 0.013, 'compound': -0.9783}\n",
      "{'neg': 0.098, 'neu': 0.851, 'pos': 0.051, 'compound': -0.296}\n",
      "{'neg': 0.039, 'neu': 0.79, 'pos': 0.171, 'compound': 0.9794}\n",
      "{'neg': 0.046, 'neu': 0.76, 'pos': 0.194, 'compound': 0.9716}\n",
      "{'neg': 0.029, 'neu': 0.677, 'pos': 0.294, 'compound': 0.9915}\n",
      "{'neg': 0.317, 'neu': 0.683, 'pos': 0.0, 'compound': -0.891}\n",
      "{'neg': 0.301, 'neu': 0.665, 'pos': 0.035, 'compound': -0.9413}\n",
      "{'neg': 0.0, 'neu': 0.968, 'pos': 0.032, 'compound': 0.0772}\n",
      "{'neg': 0.0, 'neu': 0.946, 'pos': 0.054, 'compound': 0.5859}\n",
      "{'neg': 0.081, 'neu': 0.803, 'pos': 0.116, 'compound': 0.4019}\n",
      "{'neg': 0.0, 'neu': 0.892, 'pos': 0.108, 'compound': 0.8225}\n",
      "{'neg': 0.029, 'neu': 0.802, 'pos': 0.168, 'compound': 0.9716}\n",
      "{'neg': 0.034, 'neu': 0.884, 'pos': 0.082, 'compound': 0.296}\n",
      "{'neg': 0.111, 'neu': 0.704, 'pos': 0.185, 'compound': 0.9217}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.096, 'neu': 0.623, 'pos': 0.282, 'compound': 0.9922}\n",
      "{'neg': 0.151, 'neu': 0.703, 'pos': 0.146, 'compound': -0.7906}\n",
      "{'neg': 0.059, 'neu': 0.862, 'pos': 0.079, 'compound': 0.1531}\n",
      "{'neg': 0.0, 'neu': 0.86, 'pos': 0.14, 'compound': 0.8176}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here we score each date positive, negative, or neutral. \n",
    "#Currently have 2 data sets so we will use the score_dict\n",
    "#to average out the data sets for each day.\n",
    "\n",
    "\n",
    "#We will first try without any POS filtering\n",
    "wsj_sentiment=[]\n",
    "ap_sentiment=[]\n",
    "#pos_tags_to_keep = [\"NN\",\"NNP\",\"NNS\",\"NNPS\"]\n",
    "#finds sentiment of each article in a given outlet\n",
    "def article_sentiment(filename):\n",
    "    list_sentiment=[]\n",
    "    with open(filename) as file:\n",
    "        data = file.read()\n",
    "        articles = data.split(\"--\")\n",
    "        for article in articles:\n",
    "            list_sentiment.append(check_sentiment(article))\n",
    "        return list_sentiment\n",
    "\n",
    "\n",
    "wsj_sentiment = article_sentiment(\"WSJ.txt\")\n",
    "ap_sentiment = article_sentiment(\"AP.txt\")\n",
    "\n",
    "#gets the average of the two articles based on score_dict\n",
    "daily_score=[]\n",
    "for i in range(len(wsj_sentiment)):\n",
    "    daily_score.append((score_dict[wsj_sentiment[i]]+score_dict[ap_sentiment[i]])/2) \n",
    "    \n",
    "daily_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "systematic-event",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.019, 'neu': 0.949, 'pos': 0.031, 'compound': 0.296}\n",
      "{'neg': 0.095, 'neu': 0.825, 'pos': 0.081, 'compound': -0.6249}\n",
      "{'neg': 0.026, 'neu': 0.872, 'pos': 0.102, 'compound': 0.9607}\n",
      "{'neg': 0.112, 'neu': 0.838, 'pos': 0.051, 'compound': -0.6808}\n",
      "{'neg': 0.021, 'neu': 0.865, 'pos': 0.114, 'compound': 0.9493}\n",
      "{'neg': 0.0, 'neu': 0.989, 'pos': 0.011, 'compound': 0.0772}\n",
      "{'neg': 0.042, 'neu': 0.919, 'pos': 0.039, 'compound': 0.3612}\n",
      "{'neg': 0.009, 'neu': 0.948, 'pos': 0.043, 'compound': 0.7579}\n",
      "{'neg': 0.109, 'neu': 0.772, 'pos': 0.119, 'compound': 0.2023}\n",
      "{'neg': 0.035, 'neu': 0.868, 'pos': 0.097, 'compound': 0.9709}\n",
      "{'neg': 0.054, 'neu': 0.901, 'pos': 0.045, 'compound': -0.1779}\n",
      "{'neg': 0.02, 'neu': 0.94, 'pos': 0.04, 'compound': 0.4215}\n",
      "{'neg': 0.048, 'neu': 0.862, 'pos': 0.09, 'compound': 0.9477}\n",
      "{'neg': 0.025, 'neu': 0.816, 'pos': 0.159, 'compound': 0.9782}\n",
      "{'neg': 0.067, 'neu': 0.721, 'pos': 0.212, 'compound': 0.992}\n",
      "{'neg': 0.094, 'neu': 0.699, 'pos': 0.207, 'compound': 0.9879}\n",
      "{'neg': 0.048, 'neu': 0.833, 'pos': 0.119, 'compound': 0.9153}\n",
      "{'neg': 0.032, 'neu': 0.816, 'pos': 0.152, 'compound': 0.9933}\n",
      "{'neg': 0.019, 'neu': 0.981, 'pos': 0.0, 'compound': -0.128}\n",
      "{'neg': 0.057, 'neu': 0.833, 'pos': 0.109, 'compound': 0.8957}\n",
      "{'neg': 0.02, 'neu': 0.676, 'pos': 0.304, 'compound': 0.9962}\n",
      "{'neg': 0.062, 'neu': 0.908, 'pos': 0.03, 'compound': -0.6249}\n",
      "{'neg': 0.036, 'neu': 0.907, 'pos': 0.058, 'compound': 0.8225}\n",
      "{'neg': 0.076, 'neu': 0.786, 'pos': 0.138, 'compound': 0.9231}\n",
      "{'neg': 0.014, 'neu': 0.895, 'pos': 0.091, 'compound': 0.9153}\n",
      "{'neg': 0.011, 'neu': 0.958, 'pos': 0.031, 'compound': 0.6124}\n",
      "{'neg': 0.053, 'neu': 0.848, 'pos': 0.099, 'compound': 0.9201}\n",
      "{'neg': 0.06, 'neu': 0.855, 'pos': 0.085, 'compound': 0.9042}\n",
      "{'neg': 0.06, 'neu': 0.809, 'pos': 0.131, 'compound': 0.9812}\n",
      "{'neg': 0.027, 'neu': 0.904, 'pos': 0.069, 'compound': 0.8885}\n",
      "{'neg': 0.023, 'neu': 0.842, 'pos': 0.135, 'compound': 0.9889}\n",
      "{'neg': 0.061, 'neu': 0.853, 'pos': 0.086, 'compound': 0.2023}\n",
      "{'neg': 0.058, 'neu': 0.736, 'pos': 0.206, 'compound': 0.9753}\n",
      "{'neg': 0.035, 'neu': 0.843, 'pos': 0.122, 'compound': 0.9847}\n",
      "{'neg': 0.0, 'neu': 0.946, 'pos': 0.054, 'compound': 0.5719}\n",
      "{'neg': 0.077, 'neu': 0.863, 'pos': 0.061, 'compound': -0.2732}\n",
      "{'neg': 0.027, 'neu': 0.924, 'pos': 0.049, 'compound': 0.6369}\n",
      "{'neg': 0.099, 'neu': 0.767, 'pos': 0.135, 'compound': 0.7845}\n",
      "{'neg': 0.086, 'neu': 0.788, 'pos': 0.125, 'compound': 0.296}\n",
      "{'neg': 0.057, 'neu': 0.888, 'pos': 0.056, 'compound': 0.2263}\n",
      "{'neg': 0.032, 'neu': 0.84, 'pos': 0.128, 'compound': 0.7003}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.051, 'neu': 0.753, 'pos': 0.196, 'compound': 0.9741}\n",
      "{'neg': 0.061, 'neu': 0.92, 'pos': 0.018, 'compound': -0.8074}\n",
      "{'neg': 0.067, 'neu': 0.835, 'pos': 0.099, 'compound': 0.6249}\n",
      "{'neg': 0.122, 'neu': 0.846, 'pos': 0.032, 'compound': -0.9826}\n",
      "{'neg': 0.11, 'neu': 0.798, 'pos': 0.091, 'compound': -0.4019}\n",
      "{'neg': 0.05, 'neu': 0.859, 'pos': 0.091, 'compound': 0.9485}\n",
      "{'neg': 0.027, 'neu': 0.822, 'pos': 0.151, 'compound': 0.9899}\n",
      "{'neg': 0.016, 'neu': 0.804, 'pos': 0.18, 'compound': 0.9925}\n",
      "{'neg': 0.144, 'neu': 0.856, 'pos': 0.0, 'compound': -0.891}\n",
      "{'neg': 0.167, 'neu': 0.619, 'pos': 0.214, 'compound': -0.2263}\n",
      "{'neg': 0.0, 'neu': 0.985, 'pos': 0.015, 'compound': 0.0772}\n",
      "{'neg': 0.0, 'neu': 0.946, 'pos': 0.054, 'compound': 0.8591}\n",
      "{'neg': 0.062, 'neu': 0.855, 'pos': 0.084, 'compound': 0.5994}\n",
      "{'neg': 0.0, 'neu': 0.934, 'pos': 0.066, 'compound': 0.8225}\n",
      "{'neg': 0.015, 'neu': 0.878, 'pos': 0.107, 'compound': 0.9833}\n",
      "{'neg': 0.05, 'neu': 0.875, 'pos': 0.075, 'compound': 0.34}\n",
      "{'neg': 0.081, 'neu': 0.803, 'pos': 0.116, 'compound': 0.8957}\n",
      "{'neg': 0.0, 'neu': 0.877, 'pos': 0.123, 'compound': 0.6597}\n",
      "{'neg': 0.094, 'neu': 0.701, 'pos': 0.205, 'compound': 0.9948}\n",
      "{'neg': 0.089, 'neu': 0.785, 'pos': 0.126, 'compound': 0.7717}\n",
      "{'neg': 0.052, 'neu': 0.858, 'pos': 0.09, 'compound': 0.8402}\n",
      "{'neg': 0.0, 'neu': 0.896, 'pos': 0.104, 'compound': 0.9136}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we try by filtering only nouns\n",
    "wsj_sentiment=[]\n",
    "ap_sentiment=[]\n",
    "pos_tags_to_keep = [\"NN\",\"NNP\",\"NNS\",\"NNPS\"]\n",
    "#finds sentiment of each article in a given outlet\n",
    "def article_sentiment(filename):\n",
    "    list_sentiment=[]\n",
    "    with open(filename) as file:\n",
    "        data = file.read()\n",
    "        articles = data.split(\"--\")\n",
    "        for article in articles:\n",
    "            tokenized = nltk.word_tokenize(article)\n",
    "            tagged = nltk.pos_tag(tokenized)\n",
    "            new_phrase = \"\"\n",
    "            for word in tagged:\n",
    "             \n",
    "                if word[1] in pos_tags_to_keep:\n",
    "                    new_phrase += word[0] + \" \"\n",
    "            list_sentiment.append(check_sentiment(new_phrase))\n",
    "        return list_sentiment\n",
    "\n",
    "\n",
    "wsj_sentiment = article_sentiment(\"WSJ.txt\")\n",
    "ap_sentiment = article_sentiment(\"AP.txt\")\n",
    "\n",
    "#gets the average of the two articles based on score_dict\n",
    "daily_score=[]\n",
    "for i in range(len(wsj_sentiment)):\n",
    "    daily_score.append((score_dict[wsj_sentiment[i]]+score_dict[ap_sentiment[i]])/2) \n",
    "    \n",
    "daily_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "unlimited-denver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.054, 'neu': 0.801, 'pos': 0.145, 'compound': 0.8271}\n",
      "{'neg': 0.026, 'neu': 0.67, 'pos': 0.304, 'compound': 0.9916}\n",
      "{'neg': 0.0, 'neu': 0.909, 'pos': 0.091, 'compound': 0.7351}\n",
      "{'neg': 0.145, 'neu': 0.647, 'pos': 0.208, 'compound': 0.323}\n",
      "{'neg': 0.0, 'neu': 0.805, 'pos': 0.195, 'compound': 0.8555}\n",
      "{'neg': 0.072, 'neu': 0.803, 'pos': 0.124, 'compound': 0.0772}\n",
      "{'neg': 0.0, 'neu': 0.826, 'pos': 0.174, 'compound': 0.7479}\n",
      "{'neg': 0.103, 'neu': 0.897, 'pos': 0.0, 'compound': -0.5719}\n",
      "{'neg': 0.04, 'neu': 0.809, 'pos': 0.15, 'compound': 0.8555}\n",
      "{'neg': 0.069, 'neu': 0.653, 'pos': 0.278, 'compound': 0.9896}\n",
      "{'neg': 0.063, 'neu': 0.745, 'pos': 0.192, 'compound': 0.552}\n",
      "{'neg': 0.032, 'neu': 0.729, 'pos': 0.239, 'compound': 0.8807}\n",
      "{'neg': 0.122, 'neu': 0.679, 'pos': 0.198, 'compound': 0.78}\n",
      "{'neg': 0.0, 'neu': 0.789, 'pos': 0.211, 'compound': 0.8957}\n",
      "{'neg': 0.119, 'neu': 0.712, 'pos': 0.169, 'compound': 0.6915}\n",
      "{'neg': 0.032, 'neu': 0.753, 'pos': 0.214, 'compound': 0.9246}\n",
      "{'neg': 0.099, 'neu': 0.612, 'pos': 0.289, 'compound': 0.91}\n",
      "{'neg': 0.0, 'neu': 0.852, 'pos': 0.148, 'compound': 0.8428}\n",
      "{'neg': 0.0, 'neu': 0.925, 'pos': 0.075, 'compound': 0.3818}\n",
      "{'neg': 0.053, 'neu': 0.805, 'pos': 0.142, 'compound': 0.5859}\n",
      "{'neg': 0.074, 'neu': 0.566, 'pos': 0.36, 'compound': 0.9758}\n",
      "{'neg': 0.033, 'neu': 0.873, 'pos': 0.094, 'compound': 0.6486}\n",
      "{'neg': 0.039, 'neu': 0.84, 'pos': 0.121, 'compound': 0.8288}\n",
      "{'neg': 0.047, 'neu': 0.743, 'pos': 0.21, 'compound': 0.6808}\n",
      "{'neg': 0.04, 'neu': 0.678, 'pos': 0.282, 'compound': 0.8617}\n",
      "{'neg': 0.0, 'neu': 0.97, 'pos': 0.03, 'compound': 0.3612}\n",
      "{'neg': 0.112, 'neu': 0.607, 'pos': 0.281, 'compound': 0.9356}\n",
      "{'neg': 0.04, 'neu': 0.744, 'pos': 0.216, 'compound': 0.9179}\n",
      "{'neg': 0.032, 'neu': 0.687, 'pos': 0.281, 'compound': 0.9581}\n",
      "{'neg': 0.067, 'neu': 0.806, 'pos': 0.127, 'compound': 0.6041}\n",
      "{'neg': 0.013, 'neu': 0.573, 'pos': 0.413, 'compound': 0.9954}\n",
      "{'neg': 0.098, 'neu': 0.801, 'pos': 0.101, 'compound': 0.2732}\n",
      "{'neg': 0.272, 'neu': 0.489, 'pos': 0.239, 'compound': -0.4804}\n",
      "{'neg': 0.017, 'neu': 0.711, 'pos': 0.272, 'compound': 0.9818}\n",
      "{'neg': 0.0, 'neu': 0.672, 'pos': 0.328, 'compound': 0.6566}\n",
      "{'neg': 0.0, 'neu': 0.732, 'pos': 0.268, 'compound': 0.8051}\n",
      "{'neg': 0.144, 'neu': 0.703, 'pos': 0.153, 'compound': -0.046}\n",
      "{'neg': 0.085, 'neu': 0.822, 'pos': 0.093, 'compound': -0.1779}\n",
      "{'neg': 0.264, 'neu': 0.736, 'pos': 0.0, 'compound': -0.7269}\n",
      "{'neg': 0.076, 'neu': 0.597, 'pos': 0.326, 'compound': 0.9081}\n",
      "{'neg': 0.106, 'neu': 0.894, 'pos': 0.0, 'compound': -0.3102}\n",
      "{'neg': 0.286, 'neu': 0.714, 'pos': 0.0, 'compound': -0.6808}\n",
      "{'neg': 0.106, 'neu': 0.601, 'pos': 0.293, 'compound': 0.7003}\n",
      "{'neg': 0.167, 'neu': 0.759, 'pos': 0.074, 'compound': -0.7501}\n",
      "{'neg': 0.039, 'neu': 0.697, 'pos': 0.264, 'compound': 0.9224}\n",
      "{'neg': 0.077, 'neu': 0.756, 'pos': 0.167, 'compound': 0.8625}\n",
      "{'neg': 0.06, 'neu': 0.754, 'pos': 0.187, 'compound': 0.4939}\n",
      "{'neg': 0.056, 'neu': 0.842, 'pos': 0.102, 'compound': 0.8759}\n",
      "{'neg': 0.0, 'neu': 0.596, 'pos': 0.404, 'compound': 0.9814}\n",
      "{'neg': 0.0, 'neu': 0.875, 'pos': 0.125, 'compound': 0.6771}\n",
      "{'neg': 0.18, 'neu': 0.732, 'pos': 0.088, 'compound': -0.4404}\n",
      "{'neg': 0.187, 'neu': 0.635, 'pos': 0.178, 'compound': -0.3182}\n",
      "{'neg': 0.0, 'neu': 0.81, 'pos': 0.19, 'compound': 0.7003}\n",
      "{'neg': 0.0, 'neu': 0.688, 'pos': 0.312, 'compound': 0.9184}\n",
      "{'neg': 0.057, 'neu': 0.659, 'pos': 0.284, 'compound': 0.9622}\n",
      "{'neg': 0.039, 'neu': 0.665, 'pos': 0.296, 'compound': 0.9169}\n",
      "{'neg': 0.053, 'neu': 0.685, 'pos': 0.262, 'compound': 0.9719}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.18, 'neu': 0.625, 'pos': 0.195, 'compound': 0.2023}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.073, 'neu': 0.748, 'pos': 0.178, 'compound': 0.9353}\n",
      "{'neg': 0.181, 'neu': 0.452, 'pos': 0.367, 'compound': 0.9725}\n",
      "{'neg': 0.148, 'neu': 0.66, 'pos': 0.192, 'compound': 0.7579}\n",
      "{'neg': 0.0, 'neu': 0.824, 'pos': 0.176, 'compound': 0.5719}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we try by filtering only adjectives\n",
    "wsj_sentiment=[]\n",
    "ap_sentiment=[]\n",
    "pos_tags_to_keep = [\"JJS\",\"JJR\",\"JJ\"]\n",
    "#finds sentiment of each article in a given outlet\n",
    "def article_sentiment(filename):\n",
    "    list_sentiment=[]\n",
    "    with open(filename) as file:\n",
    "        data = file.read()\n",
    "        articles = data.split(\"--\")\n",
    "        for article in articles:\n",
    "            tokenized = nltk.word_tokenize(article)\n",
    "            tagged = nltk.pos_tag(tokenized)\n",
    "            new_phrase = \"\"\n",
    "            for word in tagged:\n",
    "             \n",
    "                if word[1] in pos_tags_to_keep:\n",
    "                    new_phrase += word[0] + \" \"\n",
    "            list_sentiment.append(check_sentiment(new_phrase))\n",
    "        return list_sentiment\n",
    "\n",
    "\n",
    "wsj_sentiment = article_sentiment(\"WSJ.txt\")\n",
    "ap_sentiment = article_sentiment(\"AP.txt\")\n",
    "\n",
    "#gets the average of the two articles based on score_dict\n",
    "daily_score=[]\n",
    "for i in range(len(wsj_sentiment)):\n",
    "    daily_score.append((score_dict[wsj_sentiment[i]]+score_dict[ap_sentiment[i]])/2) \n",
    "    \n",
    "daily_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "sought-peter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.027, 'neu': 0.918, 'pos': 0.056, 'compound': 0.872}\n",
      "{'neg': 0.077, 'neu': 0.784, 'pos': 0.139, 'compound': 0.9901}\n",
      "{'neg': 0.021, 'neu': 0.88, 'pos': 0.099, 'compound': 0.9759}\n",
      "{'neg': 0.12, 'neu': 0.775, 'pos': 0.105, 'compound': -0.4033}\n",
      "{'neg': 0.017, 'neu': 0.852, 'pos': 0.131, 'compound': 0.9779}\n",
      "{'neg': 0.013, 'neu': 0.955, 'pos': 0.032, 'compound': 0.1531}\n",
      "{'neg': 0.034, 'neu': 0.903, 'pos': 0.063, 'compound': 0.8225}\n",
      "{'neg': 0.025, 'neu': 0.939, 'pos': 0.036, 'compound': 0.4215}\n",
      "{'neg': 0.097, 'neu': 0.778, 'pos': 0.125, 'compound': 0.8807}\n",
      "{'neg': 0.045, 'neu': 0.806, 'pos': 0.149, 'compound': 0.996}\n",
      "{'neg': 0.055, 'neu': 0.874, 'pos': 0.07, 'compound': 0.3818}\n",
      "{'neg': 0.023, 'neu': 0.89, 'pos': 0.087, 'compound': 0.9186}\n",
      "{'neg': 0.065, 'neu': 0.821, 'pos': 0.114, 'compound': 0.9712}\n",
      "{'neg': 0.019, 'neu': 0.81, 'pos': 0.17, 'compound': 0.9891}\n",
      "{'neg': 0.076, 'neu': 0.719, 'pos': 0.205, 'compound': 0.9937}\n",
      "{'neg': 0.083, 'neu': 0.708, 'pos': 0.209, 'compound': 0.9936}\n",
      "{'neg': 0.051, 'neu': 0.782, 'pos': 0.167, 'compound': 0.9833}\n",
      "{'neg': 0.028, 'neu': 0.822, 'pos': 0.151, 'compound': 0.9952}\n",
      "{'neg': 0.015, 'neu': 0.962, 'pos': 0.023, 'compound': 0.2111}\n",
      "{'neg': 0.056, 'neu': 0.827, 'pos': 0.117, 'compound': 0.9419}\n",
      "{'neg': 0.032, 'neu': 0.642, 'pos': 0.325, 'compound': 0.9983}\n",
      "{'neg': 0.055, 'neu': 0.9, 'pos': 0.045, 'compound': 0.0516}\n",
      "{'neg': 0.036, 'neu': 0.893, 'pos': 0.072, 'compound': 0.9485}\n",
      "{'neg': 0.072, 'neu': 0.779, 'pos': 0.15, 'compound': 0.9578}\n",
      "{'neg': 0.018, 'neu': 0.863, 'pos': 0.119, 'compound': 0.9686}\n",
      "{'neg': 0.009, 'neu': 0.961, 'pos': 0.03, 'compound': 0.7579}\n",
      "{'neg': 0.067, 'neu': 0.789, 'pos': 0.144, 'compound': 0.9816}\n",
      "{'neg': 0.058, 'neu': 0.833, 'pos': 0.109, 'compound': 0.9714}\n",
      "{'neg': 0.055, 'neu': 0.788, 'pos': 0.157, 'compound': 0.9924}\n",
      "{'neg': 0.034, 'neu': 0.885, 'pos': 0.081, 'compound': 0.9403}\n",
      "{'neg': 0.021, 'neu': 0.773, 'pos': 0.206, 'compound': 0.9981}\n",
      "{'neg': 0.069, 'neu': 0.842, 'pos': 0.089, 'compound': 0.4404}\n",
      "{'neg': 0.109, 'neu': 0.678, 'pos': 0.214, 'compound': 0.9682}\n",
      "{'neg': 0.031, 'neu': 0.814, 'pos': 0.155, 'compound': 0.9958}\n",
      "{'neg': 0.0, 'neu': 0.918, 'pos': 0.082, 'compound': 0.8176}\n",
      "{'neg': 0.06, 'neu': 0.835, 'pos': 0.106, 'compound': 0.7089}\n",
      "{'neg': 0.055, 'neu': 0.871, 'pos': 0.074, 'compound': 0.6151}\n",
      "{'neg': 0.096, 'neu': 0.776, 'pos': 0.128, 'compound': 0.7351}\n",
      "{'neg': 0.118, 'neu': 0.779, 'pos': 0.103, 'compound': -0.5994}\n",
      "{'neg': 0.063, 'neu': 0.811, 'pos': 0.126, 'compound': 0.9191}\n",
      "{'neg': 0.045, 'neu': 0.853, 'pos': 0.102, 'compound': 0.5859}\n",
      "{'neg': 0.027, 'neu': 0.946, 'pos': 0.027, 'compound': 0.0139}\n",
      "{'neg': 0.062, 'neu': 0.724, 'pos': 0.214, 'compound': 0.9826}\n",
      "{'neg': 0.084, 'neu': 0.886, 'pos': 0.03, 'compound': -0.9286}\n",
      "{'neg': 0.06, 'neu': 0.804, 'pos': 0.136, 'compound': 0.9501}\n",
      "{'neg': 0.113, 'neu': 0.828, 'pos': 0.059, 'compound': -0.9633}\n",
      "{'neg': 0.103, 'neu': 0.792, 'pos': 0.106, 'compound': 0.128}\n",
      "{'neg': 0.051, 'neu': 0.855, 'pos': 0.094, 'compound': 0.979}\n",
      "{'neg': 0.022, 'neu': 0.782, 'pos': 0.196, 'compound': 0.9966}\n",
      "{'neg': 0.014, 'neu': 0.815, 'pos': 0.171, 'compound': 0.9939}\n",
      "{'neg': 0.151, 'neu': 0.831, 'pos': 0.018, 'compound': -0.926}\n",
      "{'neg': 0.171, 'neu': 0.622, 'pos': 0.208, 'compound': -0.4939}\n",
      "{'neg': 0.0, 'neu': 0.934, 'pos': 0.066, 'compound': 0.7269}\n",
      "{'neg': 0.0, 'neu': 0.892, 'pos': 0.108, 'compound': 0.969}\n",
      "{'neg': 0.061, 'neu': 0.807, 'pos': 0.133, 'compound': 0.9719}\n",
      "{'neg': 0.01, 'neu': 0.867, 'pos': 0.123, 'compound': 0.9661}\n",
      "{'neg': 0.024, 'neu': 0.831, 'pos': 0.145, 'compound': 0.9944}\n",
      "{'neg': 0.044, 'neu': 0.891, 'pos': 0.065, 'compound': 0.34}\n",
      "{'neg': 0.106, 'neu': 0.758, 'pos': 0.136, 'compound': 0.9118}\n",
      "{'neg': 0.0, 'neu': 0.902, 'pos': 0.098, 'compound': 0.6597}\n",
      "{'neg': 0.09, 'neu': 0.711, 'pos': 0.2, 'compound': 0.9968}\n",
      "{'neg': 0.111, 'neu': 0.707, 'pos': 0.182, 'compound': 0.9819}\n",
      "{'neg': 0.071, 'neu': 0.82, 'pos': 0.109, 'compound': 0.9382}\n",
      "{'neg': 0.0, 'neu': 0.882, 'pos': 0.118, 'compound': 0.9492}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we try by filtering adjectives and nouns\n",
    "wsj_sentiment=[]\n",
    "ap_sentiment=[]\n",
    "pos_tags_to_keep = [\"JJS\",\"JJR\",\"JJ\",\"NN\",\"NNP\",\"NNS\",\"NNPS\"]\n",
    "#finds sentiment of each article in a given outlet\n",
    "def article_sentiment(filename):\n",
    "    list_sentiment=[]\n",
    "    with open(filename) as file:\n",
    "        data = file.read()\n",
    "        articles = data.split(\"--\")\n",
    "        for article in articles:\n",
    "            tokenized = nltk.word_tokenize(article)\n",
    "            tagged = nltk.pos_tag(tokenized)\n",
    "            new_phrase = \"\"\n",
    "            for word in tagged:\n",
    "             \n",
    "                if word[1] in pos_tags_to_keep:\n",
    "                    new_phrase += word[0] + \" \"\n",
    "            list_sentiment.append(check_sentiment(new_phrase))\n",
    "        return list_sentiment\n",
    "\n",
    "\n",
    "wsj_sentiment = article_sentiment(\"WSJ.txt\")\n",
    "ap_sentiment = article_sentiment(\"AP.txt\")\n",
    "\n",
    "#gets the average of the two articles based on score_dict\n",
    "daily_score=[]\n",
    "for i in range(len(wsj_sentiment)):\n",
    "    daily_score.append((score_dict[wsj_sentiment[i]]+score_dict[ap_sentiment[i]])/2) \n",
    "    \n",
    "daily_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-michael",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
