{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "artificial-realtor",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Tech News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "arabic-surveillance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "individual-delhi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4957\n"
     ]
    }
   ],
   "source": [
    "full_data = pd.read_csv(\"full_headline_data.csv\",header=0,encoding='unicode_escape')\n",
    "print(len(full_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "residential-match",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Separate positive, negative, neutral entries\n",
    "pos_lines = []\n",
    "neg_lines = []\n",
    "neu_lines = []\n",
    "for index, row in full_data.iterrows():\n",
    "    if row['rating'] == \"positive\":\n",
    "        pos_lines.append(row['text'])\n",
    "    if row['rating'] == \"negative\":\n",
    "        neg_lines.append(row['text'])\n",
    "    if row['rating'] == \"neutral\":\n",
    "        neu_lines.append(row['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mobile-carbon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1429\n",
      "610\n",
      "2917\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_lines))\n",
    "print(len(neg_lines))\n",
    "print(len(neu_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "three-rehabilitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 3715\n",
      "Development set length: 620\n",
      "Test set length: 621\n"
     ]
    }
   ],
   "source": [
    "# We train on 3/4ths of the corpus,\n",
    "# develop on 1/8th, and test on 1/8th\n",
    "\n",
    "training_set = {}\n",
    "dev_set = {}\n",
    "test_set = {}\n",
    "\n",
    "# First 3/4ths of corpus is the training set\n",
    "training_set['pos'] = pos_lines[:(int(len(pos_lines)*3/4))]\n",
    "training_set['neg'] = neg_lines[:(int(len(neg_lines)*3/4))]\n",
    "training_set['neu'] = neu_lines[:(int(len(neu_lines)*3/4))]\n",
    "\n",
    "# Next 1/8th is the dev set\n",
    "dev_set['pos'] = pos_lines[(int(len(pos_lines)*3/4)):(int(len(pos_lines)*7/8))]\n",
    "dev_set['neg'] = neg_lines[(int(len(neg_lines)*3/4)):(int(len(neg_lines)*7/8))]\n",
    "dev_set['neu'] = neu_lines[(int(len(neu_lines)*3/4)):(int(len(neu_lines)*7/8))]\n",
    "\n",
    "# Last 1/8th is the test set\n",
    "test_set['pos'] = pos_lines[(int(len(pos_lines)*7/8)):]\n",
    "test_set['neg'] = neg_lines[(int(len(neg_lines)*7/8)):]\n",
    "test_set['neu'] = neu_lines[(int(len(neu_lines)*7/8)):]\n",
    "\n",
    "print(\"Training set length: {}\".format(((len(training_set['pos']) + len(training_set['neg']) + len(training_set['neu'])))))\n",
    "print(\"Development set length: {}\".format(((len(dev_set['pos']) + len(dev_set['neg']) + len(dev_set['neu'])))))\n",
    "print(\"Test set length: {}\".format(((len(test_set['pos']) + len(test_set['neg']) + len(test_set['neu'])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "israeli-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import *\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "training_set_features = {}\n",
    "dev_set_features = {}\n",
    "test_set_features = {}\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def extract(words):\n",
    "    return dict([(stemmer.stem(word.lower()), True) for word in words.split() if word.lower() not in stop_words])\n",
    "\n",
    "training_set_features['pos'] = [(extract(f), 'pos') for f in training_set['pos']]\n",
    "training_set_features['neg'] = [(extract(f), 'neg') for f in training_set['neg']]\n",
    "training_set_features['neu'] = [(extract(f), 'neu') for f in training_set['neu']]\n",
    "\n",
    "dev_set_features['pos'] = [(extract(f), 'pos') for f in dev_set['pos']]\n",
    "dev_set_features['neg'] = [(extract(f), 'neg') for f in dev_set['neg']]\n",
    "dev_set_features['neu'] = [(extract(f), 'neu') for f in dev_set['neu']]\n",
    "\n",
    "test_set_features['pos'] = [(extract(f), 'pos') for f in test_set['pos']]\n",
    "test_set_features['neg'] = [(extract(f), 'neg') for f in test_set['neg']]\n",
    "test_set_features['neu'] = [(extract(f), 'neu') for f in test_set['neu']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "addressed-rebate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 3715 instances, testing on 621 instances\n",
      "Training complete\n",
      "accuracy: 0.7359098228663447\n",
      "Most Informative Features\n",
      "                    drop = True              neg : neu    =     52.6 : 1.0\n",
      "                    grew = True              pos : neu    =     44.2 : 1.0\n",
      "                    rose = True              pos : neu    =     42.4 : 1.0\n",
      "                    warn = True              neg : neu    =     39.8 : 1.0\n",
      "              strengthen = True              pos : neu    =     32.0 : 1.0\n",
      "                      ep = True              neg : neu    =     29.6 : 1.0\n",
      "                 decreas = True              neg : pos    =     27.3 : 1.0\n",
      "              correspond = True              neg : neu    =     26.8 : 1.0\n",
      "                  compar = True              neg : neu    =     25.1 : 1.0\n",
      "                   doubl = True              pos : neu    =     22.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "# train_features = training_set['pos'] + training_set['neg']\n",
    "# dev_features = dev_set['pos'] + dev_set['neg']\n",
    "# test_features = test_set['pos'] + test_set['neg']\n",
    "\n",
    "# print('Training on %d instances, testing on %d instances' % (len(train_features), len(test_features)))\n",
    "# classifier = NaiveBayesClassifier.train(train_features)\n",
    "# print('Training complete')\n",
    "# print('accuracy:', nltk.classify.util.accuracy(classifier, dev_features))\n",
    "# classifier.show_most_informative_features()\n",
    "\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "training_set_features = training_set_features['pos'] + training_set_features['neg'] + training_set_features['neu']\n",
    "dev_set_features = dev_set_features['pos'] + dev_set_features['neg'] + dev_set_features['neu']\n",
    "test_set_features = test_set_features['pos'] + test_set_features['neg'] + test_set_features['neu']\n",
    "\n",
    "print('Training on %d instances, testing on %d instances' % (len(training_set_features), len(test_set_features)))\n",
    "classifier = NaiveBayesClassifier.train(training_set_features)\n",
    "print('Training complete')\n",
    "print('accuracy:', nltk.classify.util.accuracy(classifier, test_set_features))\n",
    "classifier.show_most_informative_features(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "alert-claim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks the sentiment of a single string of text, discretely labeling it positive, negative, or neutral.\n",
    "# Used for finding accuracy of our training data.\n",
    "def check_sentiment_for_test(string_to_analyze):\n",
    "    feature_set = extract(string_to_analyze)\n",
    "    prob_dist = classifier.prob_classify(feature_set)\n",
    "    scores = {'pos':prob_dist.prob('pos'),'neg':prob_dist.prob('neg'),'neu':prob_dist.prob('neu')}\n",
    "    neg_neu_pos = [scores['neg'],scores['neu'],scores['pos']]\n",
    "    highest_score = neg_neu_pos.index(max(neg_neu_pos))\n",
    "    return_values = [\"negative\", \"neutral\", \"positive\"]\n",
    "    return return_values[highest_score]\n",
    "\n",
    "# Checks the sentiment of a single string of text, giving it a numerical value.\n",
    "# Used for finding numerical sentiment of our newspapers. \n",
    "# When the keyword argument \"return_all_scores\" is true, this returns the full\n",
    "# dictionary of probability scores for pos, neg, and neu. By\n",
    "# default, though, it only returns the compound score.\n",
    "\n",
    "# The compound score is just the probability of the string being positive\n",
    "# minus the probability of it being negative. This indicates, in a continuous value\n",
    "# between -1 and 1, the numerical value of the sentiment.\n",
    "def check_sentiment(string_to_analyze, return_all_scores=False):\n",
    "    feature_set = extract(string_to_analyze)\n",
    "    prob_dist = classifier.prob_classify(feature_set)\n",
    "    scores = {'pos':prob_dist.prob('pos'),'neg':prob_dist.prob('neg'),'neu':prob_dist.prob('neu')}\n",
    "    scores['compound'] = scores['pos'] - scores['neg']\n",
    "    if return_all_scores:\n",
    "        return scores\n",
    "    return scores[\"compound\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adjacent-passion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_sentiment_for_test(full_data['text'][592])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "regulated-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_pos = []\n",
    "wrong_neg = []\n",
    "wrong_neu = []\n",
    "\n",
    "# Function for testing accuracy when only certain POS tags are\n",
    "# considered in the sentiment analysis.\n",
    "def test_POS_isolations(test_dict, tags_to_keep):\n",
    "    pos_tags_to_keep = tags_to_keep\n",
    "    print(\"Testing for: {}\".format(tags_to_keep))\n",
    "    test_set = test_dict\n",
    "    filtered_word_list = []\n",
    "    for entry in test_set['pos']:\n",
    "        tokenized = nltk.word_tokenize(entry)\n",
    "        tagged = nltk.pos_tag(tokenized)\n",
    "        new_phrase = \"\"\n",
    "        for word in tagged:\n",
    "            if word[1] in pos_tags_to_keep:\n",
    "                new_phrase += word[0] + \" \"\n",
    "        filtered_word_list.append(new_phrase)\n",
    "    pos_correct = 0\n",
    "    for text in filtered_word_list:\n",
    "        if check_sentiment_for_test(text) == \"positive\":\n",
    "            pos_correct += 1\n",
    "        else:\n",
    "            wrong_pos.append((text,check_sentiment_for_test(text)))\n",
    "    pos_score = pos_correct/len(test_set['pos'])\n",
    "    print(pos_correct)\n",
    "    print(\"Correct positive guesses: {}\".format(pos_score))\n",
    "    filtered_word_list = []\n",
    "    for entry in test_set['neg']:\n",
    "        tokenized = nltk.word_tokenize(entry)\n",
    "        tagged = nltk.pos_tag(tokenized)\n",
    "        new_phrase = \"\"\n",
    "        for word in tagged:\n",
    "            if word[1] in pos_tags_to_keep:\n",
    "                new_phrase += word[0] + \" \"\n",
    "        filtered_word_list.append(new_phrase)\n",
    "    neg_correct = 0\n",
    "    for text in filtered_word_list:\n",
    "        if check_sentiment_for_test(text) == \"negative\":\n",
    "            neg_correct += 1\n",
    "        else:\n",
    "            wrong_neg.append((text,check_sentiment_for_test(text)))\n",
    "    neg_score = neg_correct/len(test_set['neg'])\n",
    "    print(neg_correct)\n",
    "    print(\"Correct negative guesses: {}\".format(neg_score))\n",
    "    filtered_word_list = []\n",
    "    for entry in test_set['neu']:\n",
    "        tokenized = nltk.word_tokenize(entry)\n",
    "        tagged = nltk.pos_tag(tokenized)\n",
    "        new_phrase = \"\"\n",
    "        for word in tagged:\n",
    "            if word[1] in pos_tags_to_keep:\n",
    "                new_phrase += word[0] + \" \"\n",
    "        filtered_word_list.append(new_phrase)\n",
    "    neu_correct = 0\n",
    "    for text in filtered_word_list:\n",
    "        if check_sentiment_for_test(text) == \"neutral\":\n",
    "            neu_correct += 1\n",
    "        else:\n",
    "            wrong_neu.append((text,check_sentiment_for_test(text)))\n",
    "    neu_score = neu_correct/len(test_set['neu'])\n",
    "    print(neu_correct)\n",
    "    print(\"Correct neutral guesses: {}\".format(neu_score))\n",
    "    \n",
    "    pos_ratio = (len(test_set['pos'])/(len(test_set['pos'])+len(test_set['neg'])+len(test_set['neu'])))\n",
    "    neg_ratio = (len(test_set['neg'])/(len(test_set['pos'])+len(test_set['neg'])+len(test_set['neu'])))\n",
    "    neu_ratio = (len(test_set['neu'])/(len(test_set['pos'])+len(test_set['neg'])+len(test_set['neu'])))\n",
    "    \n",
    "    # Precision: How many of the system's guesses were correct.\n",
    "    precision = pos_score*pos_ratio + neg_score*neg_ratio + neu_score*neu_ratio\n",
    "    print(\"Precision: {}\".format(precision))\n",
    "    \n",
    "    # Recall: How many times the system correctly identified a string as \"sentimental\" (pos/neg)\n",
    "    pos_ratio = (len(test_set['pos'])/(len(test_set['pos'])+len(test_set['neg'])))\n",
    "    neg_ratio = (len(test_set['neg'])/(len(test_set['pos'])+len(test_set['neg'])))\n",
    "    recall = pos_score*pos_ratio + neg_score*neg_ratio\n",
    "    print(\"Recall: {}\".format(recall))\n",
    "    \n",
    "    # F-Score\n",
    "    fscore = (2*precision*recall)/(precision+recall)\n",
    "    print(\"F-Score: {}\".format(fscore))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "approximate-devices",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for: ['NN', 'NNP', 'NNS', 'NNPS']\n",
      "116\n",
      "Correct positive guesses: 0.6480446927374302\n",
      "43\n",
      "Correct negative guesses: 0.5584415584415584\n",
      "274\n",
      "Correct neutral guesses: 0.7506849315068493\n",
      "Precision: 0.6972624798711755\n",
      "Recall: 0.62109375\n",
      "F-Score: 0.656977770567831\n"
     ]
    }
   ],
   "source": [
    "# Testing system when it only looks at nouns\n",
    "test_POS_isolations(test_set, [\"NN\",\"NNP\",\"NNS\",\"NNPS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "developed-athletics",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for: ['JJS', 'JJR', 'JJ']\n",
      "87\n",
      "Correct positive guesses: 0.4860335195530726\n",
      "20\n",
      "Correct negative guesses: 0.2597402597402597\n",
      "296\n",
      "Correct neutral guesses: 0.810958904109589\n",
      "Precision: 0.6489533011272142\n",
      "Recall: 0.41796875\n",
      "F-Score: 0.5084573887922649\n"
     ]
    }
   ],
   "source": [
    "# Testing system when it only looks at adjectives\n",
    "test_POS_isolations(test_set, [\"JJS\",\"JJR\",\"JJ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "revolutionary-annex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for: ['RB', 'RBR', 'RBS', 'WRB']\n",
      "16\n",
      "Correct positive guesses: 0.0893854748603352\n",
      "6\n",
      "Correct negative guesses: 0.07792207792207792\n",
      "360\n",
      "Correct neutral guesses: 0.9863013698630136\n",
      "Precision: 0.6151368760064412\n",
      "Recall: 0.0859375\n",
      "F-Score: 0.15080661079907406\n"
     ]
    }
   ],
   "source": [
    "# Testing system when it only looks at verbs\n",
    "test_POS_isolations(test_set, [\"RB\",\"RBR\",\"RBS\",\"WRB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "australian-ivory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for: ['VBZ', 'VBD', 'VBG', 'VBP', 'VBN', 'VB']\n",
      "97\n",
      "Correct positive guesses: 0.5418994413407822\n",
      "40\n",
      "Correct negative guesses: 0.5194805194805194\n",
      "312\n",
      "Correct neutral guesses: 0.8547945205479452\n",
      "Precision: 0.7230273752012882\n",
      "Recall: 0.53515625\n",
      "F-Score: 0.6150654181310962\n"
     ]
    }
   ],
   "source": [
    "# Testing system when it only looks at verbs\n",
    "test_POS_isolations(test_set, [\"VBZ\",\"VBD\",\"VBG\",\"VBP\",\"VBN\",\"VB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "prime-projector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for: ['JJS', 'JJR', 'JJ', 'NN', 'NNP', 'NNS', 'NNPS', 'VBZ', 'VBD', 'VBG', 'VBP', 'VBN', 'VB']\n",
      "146\n",
      "Correct positive guesses: 0.8156424581005587\n",
      "54\n",
      "Correct negative guesses: 0.7012987012987013\n",
      "268\n",
      "Correct neutral guesses: 0.7342465753424657\n",
      "Precision: 0.7536231884057971\n",
      "Recall: 0.78125\n",
      "F-Score: 0.7671879610504574\n"
     ]
    }
   ],
   "source": [
    "# Testing system when it only looks at adjectives, nouns, and verbs\n",
    "test_POS_isolations(test_set, [\"JJS\",\"JJR\",\"JJ\",\"NN\",\"NNP\",\"NNS\",\"NNPS\",\"VBZ\",\"VBD\",\"VBG\",\"VBP\",\"VBN\",\"VB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "infrared-debate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for: ['JJS', 'JJR', 'JJ', 'NN', 'NNP', 'NNS', 'NNPS', 'VBZ', 'VBD', 'VBG', 'VBP', 'VBN', 'VB', 'RB', 'RBR', 'RBS', 'WRB']\n",
      "146\n",
      "Correct positive guesses: 0.8156424581005587\n",
      "55\n",
      "Correct negative guesses: 0.7142857142857143\n",
      "274\n",
      "Correct neutral guesses: 0.7506849315068493\n",
      "Precision: 0.7648953301127214\n",
      "Recall: 0.78515625\n",
      "F-Score: 0.7748933735355348\n"
     ]
    }
   ],
   "source": [
    "# Testing system when it only looks at adjectives, nouns, adverbs, and verbs\n",
    "test_POS_isolations(test_set, [\"JJS\",\"JJR\",\"JJ\",\"NN\",\"NNP\",\"NNS\",\"NNPS\",\"VBZ\",\"VBD\",\"VBG\",\"VBP\",\"VBN\",\"VB\",\"RB\",\"RBR\",\"RBS\",\"WRB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "sacred-samuel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9125819153410697\n",
      "0.16040714439276055\n",
      "0.9945742153289624\n",
      "-0.045905247223948856\n",
      "0.00045884881792629406\n",
      "-0.06488061147081584\n",
      "0.713684035658864\n",
      "-0.06419074849449696\n",
      "0.07025035713257513\n",
      "0.4272990317805506\n",
      "-0.010344566634119281\n",
      "0.0005162687056892947\n",
      "0.9985301611331315\n",
      "-0.0018917708981800582\n",
      "0.07818155465650344\n",
      "-0.018639690602212666\n",
      "0.032182647852051306\n",
      "0.00017521295629551584\n",
      "0.9893340787972111\n",
      "0.03573611772163661\n"
     ]
    }
   ],
   "source": [
    "for x in full_data['text'][:20]:\n",
    "    print(check_sentiment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "apparent-australian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finds sentiment of each article in a given outlet\n",
    "def article_sentiment(filename, filterPOS):\n",
    "    list_sentiment=[]\n",
    "    with open(filename) as file:\n",
    "        data = file.read()\n",
    "        articles = data.split(\"--\")\n",
    "        for article in articles:\n",
    "            tokenized = nltk.word_tokenize(article)\n",
    "            tagged = nltk.pos_tag(tokenized)\n",
    "            if filterPOS:\n",
    "                new_phrase = \"\"\n",
    "                for word in tagged:\n",
    "                    if filterPOS:\n",
    "                        if word[1] in pos_tags_to_keep:\n",
    "                            new_phrase += word[0] + \" \"\n",
    "                list_sentiment.append(check_sentiment(new_phrase))\n",
    "            else:\n",
    "                list_sentiment.append(check_sentiment(article))\n",
    "        return list_sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "olive-truth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.25046518675204427,\n",
       " 0,\n",
       " 0,\n",
       " -0.3276732251704521,\n",
       " -0.1398721618204966,\n",
       " -0.4033507932423783,\n",
       " -0.24804574023944417,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " -0.35935813066097305,\n",
       " -0.26290498449628197,\n",
       " -0.0033266041655533774,\n",
       " -0.7973297430469115,\n",
       " -0.2665308036039439,\n",
       " 0,\n",
       " 0,\n",
       " -0.07577513497980932,\n",
       " -0.23687671012267908,\n",
       " -0.16709490914317904,\n",
       " -0.4948072842278529,\n",
       " -0.7266101656862034,\n",
       " 0,\n",
       " 0,\n",
       " -0.4519759385241029,\n",
       " -0.29464401402160684,\n",
       " -0.2527661735169947,\n",
       " -0.3429894679146513,\n",
       " -0.002044866811111823,\n",
       " 0,\n",
       " 0,\n",
       " -0.48160468799414113]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We will first try without any POS filtering\n",
    "wsj_sentiment_raw = article_sentiment(\"WSJ.txt\",False)\n",
    "ap_sentiment_raw = article_sentiment(\"AP.txt\",False)\n",
    "nyt1_sentiment_raw = article_sentiment(\"NYT1.txt\",False)\n",
    "nyt2_sentiment_raw = article_sentiment(\"NYT2.txt\",False)\n",
    "\n",
    "#gets the average sentiment of the two articles\n",
    "daily_score_raw=[]\n",
    "summage=0\n",
    "for i in range(len(wsj_sentiment_raw)):\n",
    "    if i==7:\n",
    "        daily_score_raw.append(0)\n",
    "        summage+=(wsj_sentiment_raw[i]+ap_sentiment_raw[i]+nyt1_sentiment_raw[i]+nyt2_sentiment_raw[i])/4\n",
    "    elif i%7==1 or i%7==2:\n",
    "        daily_score_raw.append(0)\n",
    "        summage+=(wsj_sentiment_raw[i]+ap_sentiment_raw[i]+nyt1_sentiment_raw[i]+nyt2_sentiment_raw[i])/4\n",
    "    elif i%7==3 and i!=10:\n",
    "        daily_score_raw.append(((wsj_sentiment_raw[i]+ap_sentiment_raw[i]+nyt1_sentiment_raw[i]+nyt2_sentiment_raw[i])/4+summage)/3) \n",
    "        summage=0\n",
    "    elif i%7==3 and i==10:\n",
    "        daily_score_raw.append(((wsj_sentiment_raw[i]+ap_sentiment_raw[i]+nyt1_sentiment_raw[i]+nyt2_sentiment_raw[i])/4+summage)/4) \n",
    "        summage=0\n",
    "    else:\n",
    "        daily_score_raw.append((wsj_sentiment_raw[i]+ap_sentiment_raw[i]+nyt1_sentiment_raw[i]+nyt2_sentiment_raw[i])/4)\n",
    "\n",
    "    \n",
    "daily_score_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "precious-words",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.4999780785362945,\n",
       " 0,\n",
       " 0,\n",
       " -0.1252962036802559,\n",
       " -0.0794361018759018,\n",
       " -0.5084940843555591,\n",
       " -0.06433996175051934,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " -0.48724051111022776,\n",
       " 0.07401682699926322,\n",
       " -0.2061337052553806,\n",
       " -0.9998213287668795,\n",
       " -3.6527506250136305e-05,\n",
       " 0,\n",
       " 0,\n",
       " -0.2829589047711973,\n",
       " -0.23464825010831655,\n",
       " 0.02663421170961741,\n",
       " -0.4102802383767612,\n",
       " -0.6299751399609517,\n",
       " 0,\n",
       " 0,\n",
       " -0.6257967595296293,\n",
       " -0.4322599684636562,\n",
       " -0.25008339495044624,\n",
       " 0.0943101104151892,\n",
       " 0.16487374144719857,\n",
       " 0,\n",
       " 0,\n",
       " -0.22004231606582195]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we try by filtering only nouns\n",
    "\n",
    "pos_tags_to_keep = [\"NN\",\"NNP\",\"NNS\",\"NNPS\"]\n",
    "\n",
    "\n",
    "\n",
    "wsj_sentiment_noun = article_sentiment(\"WSJ.txt\",True)\n",
    "ap_sentiment_noun = article_sentiment(\"AP.txt\",True)\n",
    "nyt1_sentiment_noun = article_sentiment(\"NYT1.txt\",True)\n",
    "nyt2_sentiment_noun = article_sentiment(\"NYT2.txt\",True)\n",
    "#gets the average sentiment of the two articles\n",
    "daily_score_noun=[]\n",
    "summage=0\n",
    "for i in range(len(wsj_sentiment_noun)):\n",
    "    if i==7:\n",
    "        daily_score_noun.append(0)\n",
    "        summage+=(wsj_sentiment_noun[i]+ap_sentiment_noun[i]+nyt1_sentiment_noun[i]+nyt2_sentiment_noun[i])/4\n",
    "    elif i%7==1 or i%7==2:\n",
    "        daily_score_noun.append(0)\n",
    "        summage+=(wsj_sentiment_noun[i]+ap_sentiment_noun[i]+nyt1_sentiment_noun[i]+nyt2_sentiment_noun[i])/4\n",
    "    elif i%7==3 and i!=10:\n",
    "        daily_score_noun.append(((wsj_sentiment_noun[i]+ap_sentiment_noun[i]+nyt1_sentiment_noun[i]+nyt2_sentiment_noun[i])/4+summage)/3) \n",
    "        summage=0\n",
    "    elif i%7==3 and i==10:\n",
    "        daily_score_noun.append(((wsj_sentiment_noun[i]+ap_sentiment_noun[i]+nyt1_sentiment_noun[i]+nyt2_sentiment_noun[i])/4+summage)/4) \n",
    "        summage=0\n",
    "    else:\n",
    "        daily_score_noun.append((wsj_sentiment_noun[i]+ap_sentiment_noun[i]+nyt1_sentiment_noun[i]+nyt2_sentiment_noun[i])/4)\n",
    "\n",
    "    \n",
    "daily_score_noun\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "foster-inflation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.07150961344360388, 0, 0, 0.25494774804259146, -0.19631008486710902, 0.3522218433387053, 0.40585159953858185, 0, 0, 0, 0.21102004920373585, -0.4037214218724059, 0.19697008934476412, -0.30860475534642295, -0.5002008368376816, 0, 0, 0.2899396351478672, 0.7369143188776243, 0.6518883293659777, 0.2447662454917687, 0.4880260112286672, 0, 0, -0.415101792084196, 0.2721988111038669, 0.3285695853698666, 0.5348091801122759, -0.07311594663965987, 0, 0, -0.022747441028273196]\n"
     ]
    }
   ],
   "source": [
    "#Now we try by filtering only adjectives\n",
    "\n",
    "pos_tags_to_keep = [\"JJS\",\"JJR\",\"JJ\"]\n",
    "\n",
    "\n",
    "\n",
    "wsj_sentiment_adj = article_sentiment(\"WSJ.txt\",True)\n",
    "ap_sentiment_adj = article_sentiment(\"AP.txt\",True)\n",
    "nyt1_sentiment_adj = article_sentiment(\"NYT1.txt\",True)\n",
    "nyt2_sentiment_adj = article_sentiment(\"NYT2.txt\",True)\n",
    "#gets the average sentiment of the two articles \n",
    "daily_score_adj=[]\n",
    "summage=0\n",
    "for i in range(len(wsj_sentiment_adj)):\n",
    "    if i==7:\n",
    "        daily_score_adj.append(0)\n",
    "        summage+=(wsj_sentiment_adj[i]+ap_sentiment_adj[i]+nyt1_sentiment_adj[i]+nyt2_sentiment_adj[i])/4\n",
    "    elif i%7==1 or i%7==2:\n",
    "        daily_score_adj.append(0)\n",
    "        summage+=(wsj_sentiment_adj[i]+ap_sentiment_adj[i]+nyt1_sentiment_adj[i]+nyt2_sentiment_adj[i])/4\n",
    "    elif i%7==3 and i!=10:\n",
    "        daily_score_adj.append(((wsj_sentiment_adj[i]+ap_sentiment_adj[i]+nyt1_sentiment_adj[i]+nyt2_sentiment_adj[i])/4+summage)/3) \n",
    "        summage=0\n",
    "    elif i%7==3 and i==10:\n",
    "        daily_score_adj.append(((wsj_sentiment_adj[i]+ap_sentiment_adj[i]+nyt1_sentiment_adj[i]+nyt2_sentiment_adj[i])/4+summage)/4) \n",
    "        summage=0\n",
    "    else:\n",
    "        daily_score_adj.append((wsj_sentiment_adj[i]+ap_sentiment_adj[i]+nyt1_sentiment_adj[i]+nyt2_sentiment_adj[i])/4)\n",
    "\n",
    "    \n",
    "print(daily_score_adj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-provision",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we try by filtering adjectives and nouns\n",
    "\n",
    "pos_tags_to_keep = [\"JJS\",\"JJR\",\"JJ\",\"NN\",\"NNP\",\"NNS\",\"NNPS\"]\n",
    "#finds sentiment of each article in a given outlet\n",
    "\n",
    "\n",
    "\n",
    "wsj_sentiment_adj_noun = article_sentiment(\"WSJ.txt\",True)\n",
    "ap_sentiment_adj_noun = article_sentiment(\"AP.txt\",True)\n",
    "nyt1_sentiment_adj_noun = article_sentiment(\"NYT1.txt\",True)\n",
    "nyt2_sentiment_adj_noun = article_sentiment(\"NYT2.txt\",True)\n",
    "\n",
    "#gets the average of the two articles based on score_dict\n",
    "daily_score_adj_noun=[]\n",
    "summage=0\n",
    "for i in range(len(wsj_sentiment_adj_noun)):\n",
    "    #good friday\n",
    "    if i==7:\n",
    "        daily_score_adj_noun.append(0)\n",
    "        summage+=(wsj_sentiment_adj_noun[i]+ap_sentiment_adj_noun[i]+nyt1_sentiment_adj_noun[i]+nyt2_sentiment_adj_noun[i])/4\n",
    "    #sat,sunday\n",
    "    elif i%7==1 or i%7==2:\n",
    "        daily_score_adj_noun.append(0)\n",
    "        summage+=(wsj_sentiment_adj_noun[i]+ap_sentiment_adj_noun[i]+nyt1_sentiment_adj_noun[i]+nyt2_sentiment_adj_noun[i])/4\n",
    "    #monday, average them\n",
    "    elif i%7==3 and i!=10:\n",
    "        daily_score_adj_noun.append(((wsj_sentiment_adj_noun[i]+ap_sentiment_adj_noun[i]+nyt1_sentiment_adj_noun[i]+nyt2_sentiment_adj_noun[i])/4+summage)/3) \n",
    "        summage=0\n",
    "    elif i%7 and i==10:\n",
    "        daily_score_adj_noun.append(((wsj_sentiment_adj_noun[i]+ap_sentiment_adj_noun[i]+nyt1_sentiment_adj_noun[i]+nyt2_sentiment_adj_noun[i])/4+summage)/4) \n",
    "        summage=0\n",
    "    else:\n",
    "        daily_score_adj_noun.append((wsj_sentiment_adj_noun[i]+ap_sentiment_adj_noun[i]+nyt1_sentiment_adj_noun[i]+nyt2_sentiment_adj_noun[i])/4)\n",
    "print(daily_score_adj_noun)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(daily_score_raw)\n",
    "plt.axhline(y=0, color='r', linestyle='-')\n",
    "plt.axhline(y=0.5, color='b', linestyle='-')\n",
    "plt.xlabel(\"Date (26 March - 26 April)\")\n",
    "plt.ylabel(\"Average news scores (All words)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(daily_score_noun)\n",
    "plt.axhline(y=0, color='r', linestyle='-')\n",
    "plt.axhline(y=0.5, color='b', linestyle='-')\n",
    "plt.xlabel(\"Date (26 March - 26 April)\")\n",
    "plt.ylabel(\"Average news scores (Nouns)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(daily_score_adj)\n",
    "plt.axhline(y=0, color='r', linestyle='-')\n",
    "plt.axhline(y=0.5, color='b', linestyle='-')\n",
    "plt.xlabel(\"Date (26 March - 26 April)\")\n",
    "plt.ylabel(\"Average news scores (Adjectives)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-female",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(daily_score_adj_noun)\n",
    "plt.axhline(y=0, color='r', linestyle='-')\n",
    "plt.axhline(y=0.5, color='b', linestyle='-')\n",
    "plt.xlabel(\"Date (26 March - 26 April)\")\n",
    "plt.ylabel(\"Average news scores (Adjectives & Nouns)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-albany",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "stock_prices = []\n",
    "stock = open(\"VGT Price.txt\")\n",
    "counter = 0\n",
    "for line in stock:\n",
    "    if counter == 0:\n",
    "        counter += 1\n",
    "    elif counter < 40:\n",
    "        price = line.split(\" \")\n",
    "        stock_prices.append(float(price[1].strip(\"\\n\")))\n",
    "        counter += 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "#print(stock_prices)                            \n",
    "delta_stock_prices = [(357.59-349.89)/349.89*100]\n",
    "for i in range(len(stock_prices) - 1):\n",
    "    delta_stock_prices.append((stock_prices[i+1] - stock_prices[i]) / stock_prices[i] * 100)\n",
    "print(delta_stock_prices)\n",
    "\n",
    "plt.plot(stock_prices)\n",
    "plt.xlabel(\"Date (26 March - 3 May)\")\n",
    "plt.ylabel(\"VGT Stock Price\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(delta_stock_prices)\n",
    "plt.axhline(y=0, color='r', linestyle='-')\n",
    "plt.xlabel(\"Date (26 March - 3 May)\")\n",
    "plt.ylabel(\"% Change in VGT Stock Price\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-egyptian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation\n",
    "\n",
    "y = np.array(stock_prices[:32])\n",
    "x_all = np.array(daily_score_raw)\n",
    "x_adj_noun = np.array(daily_score_adj_noun)\n",
    "x_adj = np.array(daily_score_adj)\n",
    "x_noun = np.array(daily_score_noun)\n",
    "\n",
    "r_all = np.corrcoef(x_all, y)\n",
    "r_adj_noun = np.corrcoef(x_adj_noun, y)\n",
    "r_adj = np.corrcoef(x_adj, y)\n",
    "r_noun = np.corrcoef(x_noun, y)\n",
    "print(\"Coefficient of correlation with all words:  \", r_all[0,1])\n",
    "print(\"Coefficient of correlation using nouns & adjectives: \", r_adj_noun[0,1])\n",
    "print(\"Coefficient of correlation using adjectives: \", r_adj[0,1])\n",
    "print(\"Coefficient of correlation using nouns: \", r_noun[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERROR ANALYSIS\n",
    "wrong_pos = []\n",
    "wrong_neg = []\n",
    "wrong_neu = []\n",
    "\n",
    "test_POS_isolations(test_set, [\"JJS\",\"JJR\",\"JJ\",\"NN\",\"NNP\",\"NNS\",\"NNPS\",\"VBZ\",\"VBD\",\"VBG\",\"VBP\",\"VBN\",\"VB\",\"RB\",\"RBR\",\"RBS\",\"WRB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_pos = 133\n",
    "\n",
    "false_neg = 0\n",
    "false_neu = 0\n",
    "for w in wrong_pos:\n",
    "    if w[1] == \"negative\":\n",
    "        false_neg += 1\n",
    "    else:\n",
    "        false_neu += 1\n",
    "\n",
    "labels = ['Correct positive','False negative','False neutral']\n",
    "plt.title(\"Results of sentiment analysis on positive texts\")\n",
    "plt.pie([correct_pos,false_neg,false_neu],labels=labels,autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-major",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_neg = 46\n",
    "\n",
    "false_pos = 0\n",
    "false_neu = 0\n",
    "for w in wrong_neg:\n",
    "    if w[1] == \"positive\":\n",
    "        false_pos += 1\n",
    "    else:\n",
    "        false_neu += 1\n",
    "\n",
    "labels = ['Correct negative','False positive','False neutral']\n",
    "plt.title(\"Results of sentiment analysis on negative texts\")\n",
    "plt.pie([correct_neg,false_pos,false_neu],labels=labels,autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_neu = 259\n",
    "\n",
    "false_pos = 0\n",
    "false_neg = 0\n",
    "for w in wrong_neu:\n",
    "    if w[1] == \"positive\":\n",
    "        false_pos += 1\n",
    "    else:\n",
    "        false_neg += 1\n",
    "\n",
    "labels = ['Correct neutral','False positive','False negative']\n",
    "plt.title(\"Results of sentiment analysis on neutral texts\")\n",
    "plt.pie([correct_neu,false_pos,false_neg],labels=labels,autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-vacuum",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
